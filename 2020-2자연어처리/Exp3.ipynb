{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **자연어 처리 Exp3 Report**\n",
    "                                                컴퓨터전자시스템공학부 201603727 허성일\n",
    "                                                제출일 : 2020-11-12\n",
    "                                                교수님 : 김낙현 교수님"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 로이터 뉴스 데이터에서 뉴스 주제 46개를 구하는 프로그램을 작성하고, 데이터 1~50까지 각 뉴스의 주제를 구하여 인쇄하라.\n",
    "*[문제 1]에서 주제를 텍스트로 구하는 것은 내부 데이터로는 해결되지 않으니 주제를 숫자로 적는 것으로 수정함.\n",
    "\n",
    "### 2. 로이터 뉴스 프로그램을 LSTM을 사용하지 않고 Dense layer를 이용하여 주제를 구하는 프로그램을 작성하라. 이 프로그램의 정확도와 LSTM 버전 프로그램의 정확도를 비교하라.\n",
    "\n",
    "### 3. 11월2일 수업 자료에서 IMDB 영화평 예측 프로그램에서 Dense 네트워크 구조를 1단으로 하면서 설 숫자를 16, 32, 64개로 했을 때 결과의 정확도가 어떻게 바뀌는지 분석하라.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.  \n",
    "[소스코드]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import reuters\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = reuters.load_data(num_words=1000, test_split=0.2)\n",
    "\n",
    "\n",
    "max_len = 100\n",
    "X_train = pad_sequences(X_train, maxlen=max_len) # 훈련용 뉴스 기사 패딩\n",
    "X_test = pad_sequences(X_test, maxlen=max_len) # 테스트용 뉴스 기사 패딩\n",
    "\n",
    "y_train = to_categorical(y_train) # 훈련용 뉴스 기사 레이블의 원-핫 인코딩\n",
    "y_test = to_categorical(y_test) # 테스트용 뉴스 기사 레이블의 원-핫 인코딩\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(1000, 120))\n",
    "model.add(LSTM(120))\n",
    "model.add(Dense(46, activation='softmax'))\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "                   batch_size=128, \n",
    "                   epochs=30, \n",
    "                   callbacks=[es, mc], \n",
    "                   validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.predict(X_test[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-aa844501e6fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "for i,x in enumerate(result):\n",
    "    print(\"{}번째 기사 주제 : {}\".format(i+1,np.argmax(x)),end='\\t')\n",
    "    if i%5==4:print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[설명]**  \n",
    "reuters data를 load하고 뉴스 기사를 패딩한다. 각 기사의 레이블은 원-핫 인코딩 하고 lstm 모델\n",
    "을 작성하여 모델링하고 모델 훈련시킨 다음 x_test 데이터 50개를 예측했다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.  \n",
    "[소스코드] lstm을 사용하지 않은 모델**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import reuters\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = reuters.load_data(num_words=1000, test_split=0.2)\n",
    "\n",
    "\n",
    "max_len = 100\n",
    "X_train = pad_sequences(X_train, maxlen=max_len) # 훈련용 뉴스 기사 패딩\n",
    "X_test = pad_sequences(X_test, maxlen=max_len) # 테스트용 뉴스 기사 패딩\n",
    "\n",
    "y_train = to_categorical(y_train) # 훈련용 뉴스 기사 레이블의 원-핫 인코딩\n",
    "y_test = to_categorical(y_test) # 테스트용 뉴스 기사 레이블의 원-핫 인코딩\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(46, activation='softmax'))\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "                   batch_size=128, \n",
    "                   epochs=30, \n",
    "                   callbacks=[es, mc], \n",
    "                   validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**lstm 모델과 평가**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**model : LSTM모델**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evalutate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**model1 : Dense layer모델**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.evalutate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.  \n",
    "[소스코드]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sel = 16인 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras import models \n",
    "from keras import layers\n",
    "import numpy as np\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = \timdb.load_data( num_words=10000)\n",
    "\n",
    "# 입력 텍스트 vectorization\n",
    "def vectorize_sequences(sequences, dimension=10000): \n",
    "    results = np.zeros((len(sequences), dimension)) \n",
    "    for i, sequence in enumerate(sequences): \n",
    "        results[i, sequence] = 1. \n",
    "    return results \n",
    "\n",
    "x_train = vectorize_sequences(train_data) \n",
    "x_test = vectorize_sequences(test_data)\n",
    "y_train = np.array(train_labels).astype('float32')\n",
    "y_test = np.array(test_labels).astype('float32')\n",
    "\n",
    "model16 = models.Sequential() \n",
    "model16.add(layers.Dense(16, activation='relu', input_shape=(10000,))) \n",
    "model16.add(layers.Dense(16, activation='relu')) \n",
    "model16.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model16.compile(optimizer='rmsprop', loss='binary_crossentropy', \n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model16.fit(x_train, y_train, epochs=4, batch_size=512)\n",
    "results = model16.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sel=32인 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model32 = models.Sequential() \n",
    "model32.add(layers.Dense(32, activation='relu', input_shape=(10000,))) \n",
    "model32.add(layers.Dense(16, activation='relu')) \n",
    "model32.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model32.compile(optimizer='rmsprop', loss='binary_crossentropy', \n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model32.fit(x_train, y_train, epochs=4, batch_size=512)\n",
    "results = model32.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sel=64인 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model64 = models.Sequential() \n",
    "model64.add(layers.Dense(64, activation='relu', input_shape=(10000,))) \n",
    "model64.add(layers.Dense(16, activation='relu')) \n",
    "model64.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model64.compile(optimizer='rmsprop', loss='binary_crossentropy', \n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model64.fit(x_train, y_train, epochs=4, batch_size=512)\n",
    "results = model16.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[결과]**  \n",
    "16에서 32로 증가햇을 때는 정확도가 감소하였고 64인 경우 다시 증가하였지만 16보다는 낮은\n",
    "정확도를 보였다. sel의 수가 많아지면서 모델이 더욱 복잡해지고 overfitting되는 경향을 보여주는 것으로 판단된다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
